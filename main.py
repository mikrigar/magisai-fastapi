#!/usr/bin/env python3
"""
Enhanced FastAPI RAG Service with Weaviate Query Agent and Clean Response Formatting
Uses Weaviate's intelligent query agent with beautiful response formatting
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import weaviate
from weaviate.classes.init import Auth
from weaviate.agents.query import QueryAgent
from weaviate_agents.classes import QueryAgentCollectionConfig
import httpx
import logging
import os
import time
from typing import List, Optional, Dict, Any
from dotenv import load_dotenv
from datetime import datetime
import re

load_dotenv()

app = FastAPI(title="Enhanced RAG API with Query Agent", version="2.1.0")

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
WEAVIATE_URL = os.getenv("WEAVIATE_URL")
WEAVIATE_KEY = os.getenv("WEAVIATE_API_KEY") or os.getenv("WEAVIATE_KEY")
RUNPOD_URL = os.getenv("RUNPOD_URL")
RUNPOD_KEY = os.getenv("RUNPOD_KEY")
RUNPOD_MODEL = os.getenv("RUNPOD_MODEL")

# Initialize Weaviate client and Query Agent
weaviate_client = None
query_agent = None

try:
    weaviate_client = weaviate.connect_to_weaviate_cloud(
        cluster_url=WEAVIATE_URL,
        auth_credentials=Auth.api_key(WEAVIATE_KEY)
    )
    
    # Initialize the Query Agent
    query_agent = QueryAgent(
        client=weaviate_client,
        collections=[
            QueryAgentCollectionConfig(
                name="MagisChunk",
                target_vector=["content_vector"],  # Adjust if your vector name is different
            ),
        ],
    )
    
    logger.info("‚úÖ Connected to Weaviate with Query Agent")
except Exception as e:
    logger.error(f"‚ùå Weaviate/Query Agent setup failed: {e}")


class ResponseFormatter:
    """Enhanced formatter for clean, readable responses with metadata"""
    
    @staticmethod
    def format_answer(answer: str) -> str:
        """Clean and format the answer text"""
        # Remove escape characters
        answer = answer.replace('\\n', '\n').replace('\\\'', "'").replace('\\"', '"')
        
        # Fix spacing issues
        answer = re.sub(r'\n\s*\n\s*\n+', '\n\n', answer)
        answer = re.sub(r'^\s+|\s+$', '', answer)
        
        # Add proper paragraph breaks if missing
        sentences = answer.split('. ')
        if len(sentences) > 4:
            # Group into paragraphs
            paragraphs = []
            current_para = []
            for i, sentence in enumerate(sentences):
                current_para.append(sentence + ('.' if not sentence.endswith('.') else ''))
                if len(current_para) >= 3 or i == len(sentences) - 1:
                    paragraphs.append(' '.join(current_para))
                    current_para = []
            answer = '\n\n'.join(paragraphs)
        
        return answer
    
    @staticmethod
    def format_sources(sources: List['AgentSource']) -> str:
        """Format sources with clean structure"""
        if not sources:
            return ""
        
        formatted = "\n\nüìö **Sources:**\n"
        
        for i, source in enumerate(sources[:5], 1):  # Limit to 5 sources
            # Extract key metadata
            source_name = source.source or "Unknown Source"
            confidence = source.metadata.get('confidence', source.metadata.get('distance', 0))
            method = source.metadata.get('method', 'unknown')
            
            # Format source entry
            formatted += f"\n[{i}] {source_name}\n"
            
            # Add metadata
            if isinstance(confidence, float):
                if confidence < 1:  # Likely a distance measure
                    relevance = 1 - confidence
                    formatted += f"    ‚Ä¢ Relevance: {relevance:.1%}\n"
                else:
                    formatted += f"    ‚Ä¢ Confidence: {confidence:.1%}\n"
            
            if method != 'unknown':
                formatted += f"    ‚Ä¢ Method: {method.replace('_', ' ').title()}\n"
            
            # Add content preview (first 150 chars)
            if source.content:
                preview = source.content[:150].strip()
                if len(source.content) > 150:
                    preview += "..."
                formatted += f"    ‚Ä¢ Preview: {preview}\n"
        
        return formatted
    
    @staticmethod
    def format_metadata(
        processing_time: float,
        method_used: str,
        complexity: str,
        reasoning: str,
        confidence: Optional[float] = None
    ) -> str:
        """Format response metadata"""
        formatted = "\n\nüìä **Response Details:**\n"
        
        # PROMINENT METHOD INDICATION
        if method_used == "query_agent":
            formatted += "\n **GENERATED BY: Weaviate Query Agent** (Intelligent AI reasoning)\n\n"
        elif method_used == "basic_search_with_generation":
            formatted += "\n **GENERATED BY: RunPod LLM** (Mistral model with vector search)\n\n"
        else:
            formatted += "\nüìù **GENERATED BY: Basic Vector Search** (Direct retrieval only)\n\n"
        
        # Processing time
        formatted += f"‚Ä¢ ‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\n"
        
        # Method used
        method_display = method_used.replace('_', ' ').title()
        formatted += f"‚Ä¢ üîß Method: {method_display}\n"
        
        # Complexity assessment
        complexity_emoji = {
            "simple": "üü¢",
            "moderate": "üü°", 
            "complex": "üî¥"
        }.get(complexity, "‚ö™")
        formatted += f"‚Ä¢ {complexity_emoji} Complexity: {complexity.title()}\n"
        
        # Confidence if available
        if confidence is not None:
            formatted += f"‚Ä¢ üìä Confidence: {confidence:.1%}\n"
        
        # Reasoning
        formatted += f"‚Ä¢ üí° Reasoning: {reasoning}\n"
        
        return formatted
    
    @staticmethod
    def format_full_response(
        answer: str,
        sources: List['AgentSource'],
        processing_time: float,
        method_used: str,
        complexity: str,
        reasoning: str,
        confidence: Optional[float] = None,
        question: Optional[str] = None
    ) -> str:
        """Create a complete formatted response"""
        formatted_parts = []
        
        # Add question header if provided
        if question:
            formatted_parts.append(f"üîç **Question:** {question}\n{'‚îÄ' * 60}")
        
        # Add generation method notice at the top
        if method_used == "query_agent":
            formatted_parts.append("ü§ñ **This answer was generated by: Weaviate Query Agent**")
            formatted_parts.append("*Using intelligent AI reasoning and context understanding*\n")
        elif method_used == "basic_search_with_generation":
            formatted_parts.append("üß† **This answer was generated by: RunPod LLM (Mistral)**")
            formatted_parts.append("*Using vector search + language model generation*\n")
        else:
            formatted_parts.append("üìù **This answer was generated by: Basic Vector Search**")
            formatted_parts.append("*Using direct retrieval without AI generation*\n")
        
        # Add formatted answer
        formatted_parts.append("üìñ **Answer:**")
        formatted_parts.append(ResponseFormatter.format_answer(answer))
        
        # Add sources if available
        if sources:
            formatted_parts.append(ResponseFormatter.format_sources(sources))
        
        # Add metadata
        formatted_parts.append(ResponseFormatter.format_metadata(
            processing_time, method_used, complexity, reasoning, confidence
        ))
        
        return '\n'.join(formatted_parts)


class QueryRequest(BaseModel):
    question: str
    user: str
    force_method: Optional[str] = None  # "agent", "basic", or None for auto-detect
    max_chunks: Optional[int] = 5
    format_response: Optional[bool] = True  # Enable formatting by default

class AgentSource(BaseModel):
    content: str
    source: str
    metadata: Dict[str, Any] = {}
    
    def to_display_dict(self) -> Dict[str, Any]:
        """Convert to display format with enhanced metadata"""
        return {
            "content": self.content,
            "source": self.source,
            "metadata": self.metadata,
            "timestamp": datetime.now().isoformat()
        }

class QueryResponse(BaseModel):
    answer: str
    sources: List[AgentSource]
    processing_time: float
    method_used: str  # "query_agent" or "basic_search"
    complexity_assessment: str  # "simple", "moderate", "complex"
    reasoning: str  # Why this method was chosen
    agent_confidence: Optional[float] = None
    formatted_response: Optional[str] = None  # Full formatted response
    
    def format_for_display(self, question: Optional[str] = None) -> str:
        """Generate formatted display version"""
        return ResponseFormatter.format_full_response(
            answer=self.answer,
            sources=self.sources,
            processing_time=self.processing_time,
            method_used=self.method_used,
            complexity=self.complexity_assessment,
            reasoning=self.reasoning,
            confidence=self.agent_confidence,
            question=question
        )

@app.get("/")
async def root():
    return {
        "service": "Enhanced RAG API with Query Agent",
        "version": "2.1.0",
        "features": [
            "weaviate_query_agent", 
            "intelligent_retrieval", 
            "enhanced_reasoning",
            "beautiful_formatting",
            "metadata_display"
        ],
        "status": "running"
    }

@app.get("/models")
async def check_available_models():
    """Check what models are available on the RunPod endpoint"""
    
    if not RUNPOD_URL or not RUNPOD_KEY:
        raise HTTPException(status_code=503, detail="RunPod not configured")
    
    try:
        # Try to get models list
        models_url = RUNPOD_URL.replace('/v1/chat/completions', '/v1/models')
        
        async with httpx.AsyncClient() as client:
            response = await client.get(
                models_url,
                headers={
                    "Authorization": f"Bearer {RUNPOD_KEY}",
                    "Content-Type": "application/json"
                },
                timeout=10.0
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                return {
                    "error": f"Status {response.status_code}",
                    "current_model": RUNPOD_MODEL,
                    "configured_url": RUNPOD_URL,
                    "response": response.text
                }
                
    except Exception as e:
        return {
            "error": str(e),
            "current_model": RUNPOD_MODEL,
            "configured_url": RUNPOD_URL
        }

@app.post("/test-model")
async def test_model_connection():
    """Test the current model configuration"""
    
    if not RUNPOD_URL or not RUNPOD_KEY:
        raise HTTPException(status_code=503, detail="RunPod not configured")
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                RUNPOD_URL,
                json={
                    "model": RUNPOD_MODEL,
                    "messages": [{"role": "user", "content": "Hello, please respond with 'Test successful'"}],
                    "max_tokens": 10,
                    "temperature": 0.1
                },
                headers={
                    "Authorization": f"Bearer {RUNPOD_KEY}",
                    "Content-Type": "application/json"
                },
                timeout=15.0
            )
            
            return {
                "status_code": response.status_code,
                "model_used": RUNPOD_MODEL,
                "url_used": RUNPOD_URL,
                "response": response.json() if response.status_code == 200 else response.text
            }
            
    except Exception as e:
        return {
            "error": str(e),
            "model_used": RUNPOD_MODEL,
            "url_used": RUNPOD_URL
        }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "weaviate_connected": weaviate_client is not None and weaviate_client.is_ready(),
        "query_agent_ready": query_agent is not None,
        "runpod_configured": bool(RUNPOD_URL and RUNPOD_KEY),
        "runpod_model": RUNPOD_MODEL,
        "intelligent_routing": True,
        "agent_collections": ["MagisChunk"] if query_agent else [],
        "formatting_enabled": True
    }

async def assess_query_complexity(question: str) -> Dict[str, str]:
    """Use Mistral to assess query complexity and recommend processing method"""
    
    if not RUNPOD_URL or not RUNPOD_KEY:
        # Fallback to simple heuristics if no LLM available
        return simple_complexity_heuristic(question)
    
    complexity_prompt = f"""Analyze this question and determine its complexity for a theological/philosophical knowledge system:

Question: "{question}"

Classify the complexity and recommend the best processing method:

COMPLEXITY LEVELS:
- SIMPLE: Basic factual questions, definitions, yes/no questions, single concept queries
- MODERATE: Questions requiring some reasoning but straightforward to answer
- COMPLEX: Multi-faceted questions, requires deep reasoning, philosophical analysis, connecting multiple concepts, comparative analysis

PROCESSING METHODS:
- basic_search: For simple/moderate questions that can be answered with direct retrieval + basic generation
- query_agent: For complex questions requiring intelligent reasoning, context understanding, and sophisticated retrieval

Respond in exactly this format:
COMPLEXITY: [simple/moderate/complex]
METHOD: [basic_search/query_agent]
REASONING: [One sentence explanation of why this method is best]"""

    try:
        async with httpx.AsyncClient() as client:
            # Use the exact URL from environment
            logger.info(f"Making request to: {RUNPOD_URL}")
            
            response = await client.post(
                RUNPOD_URL,
                json={
                    "model": RUNPOD_MODEL,
                    "messages": [{"role": "user", "content": complexity_prompt}],
                    "max_tokens": 150,
                    "temperature": 0.1,  # Low temperature for consistent analysis
                    "stream": False
                },
                headers={
                    "Authorization": f"Bearer {RUNPOD_KEY}",
                    "Content-Type": "application/json"
                },
                timeout=15.0
            )
            
            logger.info(f"Response status: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                if result.get("choices") and result["choices"][0].get("message"):
                    analysis = result["choices"][0]["message"]["content"].strip()
                    return parse_complexity_response(analysis)
                elif result.get("response"):
                    analysis = result["response"].strip()
                    return parse_complexity_response(analysis)
            else:
                logger.error(f"RunPod error response: {response.text}")
            
            logger.warning("LLM complexity assessment failed, using heuristics")
            return simple_complexity_heuristic(question)
            
    except Exception as e:
        logger.error(f"Complexity assessment error: {e}")
        return simple_complexity_heuristic(question)

def parse_complexity_response(analysis: str) -> Dict[str, str]:
    """Parse the LLM response for complexity assessment"""
    
    complexity = "moderate"
    method = "basic_search"
    reasoning = "Default fallback decision"
    
    try:
        lines = analysis.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith("COMPLEXITY:"):
                complexity = line.split(":", 1)[1].strip().lower()
            elif line.startswith("METHOD:"):
                method = line.split(":", 1)[1].strip().lower()
            elif line.startswith("REASONING:"):
                reasoning = line.split(":", 1)[1].strip()
    except Exception as e:
        logger.error(f"Failed to parse complexity response: {e}")
    
    return {
        "complexity": complexity,
        "method": method,
        "reasoning": reasoning
    }

def simple_complexity_heuristic(question: str) -> Dict[str, str]:
    """Fallback heuristic-based complexity assessment"""
    
    question_lower = question.lower()
    
    # Simple question indicators
    simple_indicators = [
        "what is", "who is", "when did", "where is", "define", "meaning of",
        "yes or no", "true or false", "is it", "does it", "can you"
    ]
    
    # Complex question indicators  
    complex_indicators = [
        "why does", "how does", "explain the relationship", "compare", "analyze",
        "what are the implications", "philosophical", "theological significance",
        "consciousness", "quantum", "existence", "metaphysical", "epistemological",
        "prove that", "demonstrate", "argue for", "justify"
    ]
    
    if any(indicator in question_lower for indicator in complex_indicators):
        return {
            "complexity": "complex",
            "method": "query_agent", 
            "reasoning": "Question contains complex philosophical/theological concepts requiring intelligent analysis"
        }
    elif any(indicator in question_lower for indicator in simple_indicators):
        return {
            "complexity": "simple",
            "method": "basic_search",
            "reasoning": "Simple factual question suitable for direct retrieval"
        }
    else:
        return {
            "complexity": "moderate", 
            "method": "basic_search",
            "reasoning": "Moderate complexity question suitable for enhanced retrieval"
        }

async def query_with_agent(question: str) -> Dict[str, Any]:
    """Use Weaviate Query Agent for intelligent retrieval and reasoning"""
    
    if not query_agent:
        raise HTTPException(status_code=503, detail="Query Agent not available")
    
    try:
        # Run the query agent
        result = query_agent.run(question)
        
        # Extract the answer and sources from agent result
        # The agent typically returns a structured response
        answer = str(result.answer) if hasattr(result, 'answer') else str(result)
        
        # Try to extract sources if available
        sources = []
        if hasattr(result, 'sources') and result.sources:
            for idx, source in enumerate(result.sources):
                # Extract metadata for each source
                metadata = {}
                if hasattr(source, 'metadata'):
                    metadata = source.metadata
                elif hasattr(source, '__dict__'):
                    metadata = {k: v for k, v in source.__dict__.items() if not k.startswith('_')}
                
                # Add agent-specific metadata
                metadata.update({
                    "method": "intelligent_query_agent",
                    "index": idx,
                    "agent_collection": "MagisChunk"
                })
                
                agent_source = AgentSource(
                    content=getattr(source, 'content', str(source)),
                    source=getattr(source, 'source', f'Query Agent Result #{idx+1}'),
                    metadata=metadata
                )
                sources.append(agent_source)
        else:
            # If no explicit sources, create one from the result
            sources.append(AgentSource(
                content=answer,
                source="Weaviate Query Agent",
                metadata={
                    "method": "intelligent_query_agent",
                    "collection": "MagisChunk"
                }
            ))
        
        logger.info(f"‚úÖ Query Agent processed successfully with {len(sources)} sources")
        return {
            "answer": answer,
            "sources": sources,
            "confidence": getattr(result, 'confidence', 0.9)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Query Agent failed: {e}")
        raise HTTPException(status_code=500, detail=f"Query Agent error: {str(e)}")

async def basic_search_with_generation(question: str, max_chunks: int = 5) -> Dict[str, Any]:
    """Basic vector search + Mistral generation for simple/moderate questions"""
    
    if not weaviate_client:
        raise HTTPException(status_code=503, detail="Weaviate not connected")
    
    try:
        # Get relevant chunks
        collection = weaviate_client.collections.get("MagisChunk")
        
        response = collection.query.near_text(
            query=question,
            limit=max_chunks,
            return_metadata=["distance"],
            return_properties=["content", "sourceFile", "topic", "agent"]
        )
        
        sources = []
        context_parts = []
        
        for i, obj in enumerate(response.objects, 1):
            content = obj.properties.get("content", "")
            source_file = obj.properties.get("sourceFile", "Unknown")
            topic = obj.properties.get("topic", "")
            agent_name = obj.properties.get("agent", "")
            
            # Build metadata
            metadata = {
                "distance": obj.metadata.distance if hasattr(obj.metadata, 'distance') else 1.0,
                "method": "basic_search_with_generation",
                "chunk_index": i
            }
            
            if topic:
                metadata["topic"] = topic
            if agent_name:
                metadata["agent"] = agent_name
                
            source = AgentSource(
                content=content,
                source=source_file,
                metadata=metadata
            )
            sources.append(source)
            context_parts.append(f"[{i}] {content}")
        
        if not sources:
            return {
                "answer": "No relevant information found to answer your question.",
                "sources": [],
                "confidence": 0.0
            }
        
        # Generate answer using Mistral
        context = "\n\n".join(context_parts)
        
        generation_prompt = f"""Question: {question}

Context from theological sources:
{context}

Please provide a clear, accurate answer based on the context above. Use the information provided to give a helpful response."""

        # Generate with Mistral
        async with httpx.AsyncClient() as client:
            logger.info(f"Making generation request to: {RUNPOD_URL}")
            
            response = await client.post(
                RUNPOD_URL,
                json={
                    "model": RUNPOD_MODEL,
                    "messages": [{"role": "user", "content": generation_prompt}],
                    "max_tokens": 800,
                    "temperature": 0.3,
                    "stream": False
                },
                headers={
                    "Authorization": f"Bearer {RUNPOD_KEY}",
                    "Content-Type": "application/json"
                },
                timeout=30.0
            )
            
            logger.info(f"Generation response status: {response.status_code}")
            
            answer = "Based on the retrieved sources, I can provide relevant information, but please try again for a complete response."
            
            if response.status_code == 200:
                result = response.json()
                if result.get("choices") and result["choices"][0].get("message"):
                    answer = result["choices"][0]["message"]["content"].strip()
                elif result.get("response"):
                    answer = result["response"].strip()
            else:
                logger.error(f"Generation error response: {response.text}")
        
        logger.info(f"‚úÖ Basic search + generation completed with {len(sources)} sources")
        return {
            "answer": answer,
            "sources": sources,
            "confidence": 0.8 if sources else 0.0
        }
        
    except Exception as e:
        logger.error(f"‚ùå Basic search + generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Search and generation error: {str(e)}")

async def basic_vector_search(question: str, max_chunks: int = 5) -> Dict[str, Any]:
    """Fallback to basic vector search"""
    
    if not weaviate_client:
        raise HTTPException(status_code=503, detail="Weaviate not connected")
    
    try:
        collection = weaviate_client.collections.get("MagisChunk")
        
        response = collection.query.near_text(
            query=question,
            limit=max_chunks,
            return_metadata=["distance"],
            return_properties=["content", "sourceFile", "topic", "agent"]
        )
        
        sources = []
        for idx, obj in enumerate(response.objects):
            metadata = {
                "distance": obj.metadata.distance if hasattr(obj.metadata, 'distance') else 1.0,
                "method": "basic_vector_search",
                "chunk_index": idx
            }
            
            # Add additional metadata if available
            if obj.properties.get("topic"):
                metadata["topic"] = obj.properties["topic"]
            if obj.properties.get("agent"):
                metadata["agent"] = obj.properties["agent"]
                
            source = AgentSource(
                content=obj.properties.get("content", ""),
                source=obj.properties.get("sourceFile", "Unknown"),
                metadata=metadata
            )
            sources.append(source)
        
        # Create basic answer from first source
        answer = f"Based on the retrieved information: {sources[0].content[:500]}..." if sources else "No relevant information found."
        
        logger.info(f"‚úÖ Basic search retrieved {len(sources)} sources")
        return {
            "answer": answer,
            "sources": sources,
            "confidence": 0.7 if sources else 0.0
        }
        
    except Exception as e:
        logger.error(f"‚ùå Basic search failed: {e}")
        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")

@app.post("/ask", response_model=QueryResponse)
async def ask_question(request: QueryRequest):
    """Main query endpoint with intelligent complexity-based routing and enhanced formatting"""
    
    start_time = time.time()
    logger.info(f"Query from {request.user}: {request.question}")
    
    try:
        # Assess complexity and choose method (unless forced)
        if request.force_method:
            if request.force_method == "agent":
                assessment = {"complexity": "complex", "method": "query_agent", "reasoning": "Forced to use query agent"}
            else:
                assessment = {"complexity": "simple", "method": "basic_search", "reasoning": "Forced to use basic search"}
        else:
            assessment = await assess_query_complexity(request.question)
        
        logger.info(f"Complexity assessment: {assessment['complexity']} -> {assessment['method']}")
        
        # Route to appropriate method
        if assessment["method"] == "query_agent" and query_agent:
            result = await query_with_agent(request.question)
            method_used = "query_agent"
        else:
            # Use basic search + Mistral generation for simple/moderate questions
            result = await basic_search_with_generation(request.question, request.max_chunks)
            method_used = "basic_search_with_generation"
        
        # Create response
        response = QueryResponse(
            answer=result["answer"],
            sources=result["sources"],
            processing_time=time.time() - start_time,
            method_used=method_used,
            complexity_assessment=assessment["complexity"],
            reasoning=assessment["reasoning"],
            agent_confidence=result.get("confidence")
        )
        
        # Add formatted response if requested
        if request.format_response:
            response.formatted_response = response.format_for_display(request.question)
        
        return response
        
    except Exception as e:
        logger.error(f"‚ùå Query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ask-agent")
async def ask_with_agent_only(request: QueryRequest):
    """Endpoint that forces use of Query Agent only"""
    
    if not query_agent:
        raise HTTPException(status_code=503, detail="Query Agent not available")
    
    # Force agent usage
    request.force_method = "agent"
    return await ask_question(request)

@app.post("/ask-basic")
async def ask_with_basic_search_only(request: QueryRequest):
    """Endpoint that forces use of basic search + generation only"""
    
    # Force basic search
    request.force_method = "basic"
    return await ask_question(request)

@app.get("/format-example")
async def show_format_example():
    """Show an example of the enhanced formatting"""
    
    # Create example data
    example_sources = [
        AgentSource(
            content="The concept of consciousness in quantum mechanics suggests that observation plays a fundamental role in collapsing wave functions.",
            source="quantum_consciousness.txt",
            metadata={
                "distance": 0.15,
                "method": "query_agent",
                "topic": "Quantum Consciousness",
                "agent": "Science and Faith"
            }
        ),
        AgentSource(
            content="Many physicists argue that consciousness is required for the measurement problem in quantum mechanics.",
            source="physics_and_mind.pdf", 
            metadata={
                "distance": 0.22,
                "method": "query_agent",
                "topic": "Physics and Mind",
                "agent": "Science and Faith"
            }
        )
    ]
    
    example_response = QueryResponse(
        answer="Yes, there is substantial evidence suggesting a mind behind quantum mechanics. The measurement problem in quantum mechanics indicates that conscious observation plays a fundamental role in collapsing wave functions and determining physical reality. This has led many physicists and philosophers to argue that consciousness is not merely an emergent property of matter, but rather a fundamental aspect of reality itself.",
        sources=example_sources,
        processing_time=2.34,
        method_used="query_agent",
        complexity_assessment="complex",
        reasoning="Question involves deep philosophical and scientific concepts requiring intelligent analysis",
        agent_confidence=0.92
    )
    
    formatted = example_response.format_for_display("Is there a mind behind quantum mechanics?")
    
    return {
        "example_question": "Is there a mind behind quantum mechanics?",
        "raw_response": example_response.dict(),
        "formatted_response": formatted,
        "formatting_enabled": True
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
